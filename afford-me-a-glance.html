<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />

    <title>MyPortfolio -- Dan DeWitz</title>
    <meta content="" name="description" />
    <meta content="" name="keywords" />

    <!-- Favicons -->
    <link href="assets/img/favicon.png" rel="icon" />
    <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon" />

    <!-- Google Fonts -->
    <link
      href="https://fonts.googleapis.com/css?family=https://fonts.googleapis.com/css?family=Inconsolata:400,500,600,700|Raleway:400,400i,500,500i,600,600i,700,700i"
      rel="stylesheet"
    />

    <!-- Vendor CSS Files -->
    <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" />
    <link href="assets/vendor/icofont/icofont.min.css" rel="stylesheet" />
    <link href="assets/vendor/aos/aos.css" rel="stylesheet" />
    <link href="assets/vendor/line-awesome/css/line-awesome.min.css" rel="stylesheet" />
    <link href="assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet" />

    <!-- Template Main CSS File -->
    <link href="assets/css/style.css" rel="stylesheet" />

    <!-- =======================================================
  * Template Name: MyPortfolio - v2.2.0
  * Template URL: https://bootstrapmade.com/myportfolio-bootstrap-portfolio-website-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
    <style media="screen">
      html, body {
      overflow-x: hidden;
      }
    </style>


  </head>

  <body>

    <nav class="navbar navbar-light custom-navbar">
      <div class="container">
        <!-- <a class="navbar-brand" href="index.html?dc=data-science">Portfolio.</a> -->
        <a class="navbar-brand" href="index.html">MyPortfolio.</a>

        <!-- <a href="#" class="burger" data-toggle="collapse" data-target="#main-navbar">
        <span></span>
      </a> -->
      </div>
    </nav>

    <main id="main">
      <section class="section">

        <div class="site-section pb-0">
          <div class="container">

            <div class="row">
              <div class="col-md-12 mb-5" data-aos="" data-aos-delay="">
                <h2 class="text-left ">
                  Afford me a glance: gaze strategies in rapid perception of hand-object actions
                </h2>
                <h5 class="mb-2 text-muted">Authors: D. DeWitz, L. Wheaton, and N. Natraj</h6>
                <h5 class="mb-2 text-muted">Laboratory of Lewis A. Wheaton, Ph.D., Georgia Institute of Technology</h6>
                <h5 class="mb-2 text-muted">Research Mentor: Nikhilesh Natraj, Ph.D. Student, Georgia Institute of Technology</h6>
                <h5 class="mb-2 text-muted">August 2012</h6>
              </div>
            </div>

            <div class="row">
              <div class="col-md-12 mb-5" data-aos="" data-aos-delay="">
                <h3 class="text-left ">
                  Abstract
                </h3>
                <p>
Fixations and saccades play an intriguing role in decoding visual scenes, and the speeds at which humans process visual scenes is astonishing. Prior neuroimaging work has shown that visual processing of action scenes involving tool use occurs within 200ms with specific high order neural substrates (temporal and parietal areas). Exactly how the image features are encoded to arrive at perceptual decisions remains a mystery. Here, gaze patterns were measured via eye tracking as participants judged rapid tool-object action scenes with decisions on tool-object appropriateness. The hypothesis was that humans would minimize eye movements to rely on extrafoveal information. In the first experiment, when images were presented only for 500ms, participants were accurate on 80% of trials and initiated saccades in 78% of trials. Also, the latency in initiating a saccade significantly correlated with an increasing probability of a first saccade target to the manipulative end of the tool. This strategy was not used in a second experiment when images were randomized between time intervals of 500 and 2000ms. In the latter paradigm, though accuracy was consistently > 80%, image presentation of 2000ms elicited significantly quicker behavioral responses.
                </p>
            </div>
          </div>

            <div class="row">
              <div class="col-md-12 mb-5" data-aos="" data-aos-delay="">
                <h3 class="text-left ">
                  Introduction
                </h3>
                <p>
                  Human gaze patterns and fixations are constantly shifting; even now, while reading this text your eyes seldom remain still.  The goal of a saccade–-a rapid change in gaze due to a combination of eye and head movements–-is to orient a stimulus of interest onto the fovea of the eye (Gollisch, 2009).  On average, humans initiate saccades 3 to 4 times per second; immediately following a saccade, visual acuity is sharpest due to strong retinal input from sudden image changes, and acuity decays until initiation of another saccade (Hamm, Dyckman, Ethridge, McDowell, & Clementz, 2010).  Hence, our eyes act like a high resolution camera taking “snapshots” of our environment (Shadmehr, 2010).  Within the retina of the eye, the fovea accounts for the highest concentration of cones: a photoreceptor and primary substrate of visual acuity in humans.  As photons interact further away from the fovea, visual acuity drops exponentially in proportion to a receding number of cones.
                </p>
                <p>
                  A fascinating aspect of visual processing is that certain features of visual scenes elicit activations in category specific cortical regions (Lewis, 2006).  For example, activations while viewing familiar tools are specific to parieto-frontal cortical regions, which includes ventral pre-motor cortex, a region involved in motor planning.  On the other hand, when viewing unfamiliar tools and non-manipulable objects, temporal occipital cortex is activated   (Roberts & Humphreys, 2010; Kellenbach, Matthew Brett & Patterson 2003).  Parieto-frontal activations associated with viewing familiar tools are believed to represent retrieval of stored information pertaining familiar tool-use actions (Chao & Martin, 2002).  Meaning, that when viewing tools not only are structural features visually encoded, but all possible actions that a tool can perform are also involved in its visual representation (Ellis & Tucker, 2000).  Mirror neurons, a class of neurons activated both when viewing goal directed movements and when executing the same movements, are potential a mechanism of visually encoding a tool’s affordances-–all possible actions that a particular tool allows for (Rizzolatti, 2001; Borghi, Flumini, Natraj & Wheaton, 2012).  Working in conjunction, these systems allow for the understanding and simulation of observed actions, such as viewing tool-use action scenes.
                </p>
                <p>
                  Tool pairings (correct vs. incorrect) influence the timing of cortical activations.  Incorrect tool pairings evoke quicker neural activations, occurring within 200 ms, relative to activation occurring at approximately 400 ms when viewing correct tool pairings (Mizelle & Wheaton, 2010).  The way a tool is grasped also influences cortical processing.  A tool grasped in a manipulative (to move) fashion elicits unique spatial temporal neural activations.  In addition, a manipulative grasp also effects response times when judging the correctness of a tool-object scene.  When a tool is grasped in a manipulative posture it took participants a significantly greater amount of time to respond  (Borghi, Flumini, Natraj & Wheaton, 2012).  Prior eye-tracking work in our lab also emphasized the influence of grasp.  Participants typically made their first saccade to the object while passively viewing images consisting of tool-object scenes.  This gaze pattern held true in all conditions.
                </p>
                <p>
                  Though an amassing amount of neuroimaging evidence is beginning to identify specific cortical networks involved in processing tool-object scenes, how specific features of these scenes are quickly decoded remains unknown.  For example, if participants are given only 500 ms to view tool-object images and decide if the tool-object pairing is correct, are certain features deemed to be of higher value and provide a richer signal?  Here, we aim to identify how humans parse tool-object action scenes to arrive at quick and accurate perceptual decisions regarding tool-object correctness, using eye-tracking.  Given the speed at which visual processing occurs, we hypothesize that response accuracy will be significantly greater than chance when tool-object images are only shown for 500ms.  Additionally, participants will rely significantly more on extrafoveal information; when gaze does shift, the time to initiate a saccade and first saccade target will be influenced by the type of hand grasp.   Lastly, we predict gaze strategy will influence behavioral responses, i.e., the more saccades made the longer it will take to generate a response.
                </p>
            </div>
          </div>


            <div class="row">
              <div class="col-md-12 mb-2" data-aos="" data-aos-delay="">
                <h3 class="text-left ">
                  Methods
                </h3>
                <h5 class="text-muted">Participants</h5>
                <p>
                  Nine students recruited from college campuses of a large metropolitan area (mean age = 21.5) participated in this experiment that lasted approximately an hour.  All were right handed by self-report with normal to corrected-to-normal vision and naive as to the purpose of the experiment.  Monetary compensation was awarded to participants, and experimental procedures were approved by Georgia Tech IRB.
                </p>
            </div>
          </div>
          <div class="row">
            <div class="col-md-12 mb-2" data-aos="" data-aos-delay="">
              <h5 class="text-muted">Experimental set-up</h5>
              <p>
                Experimentation took place in a quiet and dimly lit room.  Instruction was provided regarding the following: (1) eye movements will be recorded while viewing tool-object scenes; (2) fixate on the fixation cross when it appears; (3) a judgment of the projected tool-object scene regarding “tool object relatedness” is to be made via button press; (4) some tool-object images will be presented for an extremely short amount of time (500 ms), responses are not restricted to only when the tool-object image is on screen but can be made during the fixation cross time interval (randomized between 2-4 s) after the tool-object image disappears.
              </p>
          </div>
        </div>
        <div class="row">
          <div class="col-md-12 mb-2" data-aos="" data-aos-delay="">
            <h5 class="text-muted">The judgment task</h5>
            <p>
              Participants were asked to respond to the following question via button press: Is the relatedness of the tool object scene correct?  The following explanation and example was given: “Is the tool capable of performing a familiar tool-object action goal.  For example, a screw driver and a screw is a correct tool object pairing.  But a screw driver and a bag of coffee are an incorrect tool-object pairing.”  Every participant selected “Yes” with their left hand, and “No” with their right on a handheld controller.  Responses were time locked and recorded with neuro-scan software.
            </p>
        </div>
      </div>

    <div class="row align-items-center no-gutters mb-4" id="ourstory" >
        <div class="col-xl-6 col-lg-6">
            <div class="text-left text-lg-left">
                <h5 class="text-muted">Stimuli</h5>
                <p class="mb-0">
            One hundred and twenty grey scale tool-object images consisting of 12 different conditions were used during experimentation.  Images varied in tool-object relatedness, and possible options included correct, incorrect, or thematic (spatial)  tool-object relationships.  In addition to these three variables, hand position also varied and images could contain no hand, a manipulative grasp (to move), a functional grasp or a static hand (See Figure,1).
                </p>
            </div>
        </div>
        <div class="col-xl-6 col-lg-6 mt-4">
          <img class="img-fluid mb-3 mb-lg-0" src="assets/img/hand-condition.png" alt=""/>
          <p class="text-right pr-2"><b>Figure 1</b></p>
        </div>
    </div>

    <div class="row align-items-center no-gutters mb-4" id="ourstory">
        <div class="col-lg-6 order-lg-first"><img class="img-fluid mb-3 mb-lg-0" src="assets/img/freddy-experiment.png" alt=""/>
          <p class="text-right pr-5"><b>Figure 2</b></p>
        </div>
        <div class="col-lg-6 order-first mt-2">
            <div class="text-left text-lg-left">
                <h5 class="text-muted pl-md-3">Apparatus</h5>
                <p class="mb-0 pl-md-3">
                Eye movements were measured with a SensoMotoric Instruments (SMI) eye tracker running at (50 Hz -sampling issue) and x,y data points were recorded with (iview x software and custom laptop computer) (See Figure, 2).   A chin rest was used to minimize head movements and maintain a consistent distance 58 of inches from a 42 inch LCD-TV screen.   Stimuli with a resolution of 1920 x 1080 pixels were projected real time through Stim software with custom code written in Mat-lab.
                </p>
            </div>
        </div>
    </div>

    <div class="row align-items-center no-gutters mb-4" id="ourstory" >
        <div class="col-lg-12">
            <div class="text-left text-lg-left">
                <h5 class="text-muted">Protocol</h5>
                <p class="mb-0">
Two experimental paradigms (a & b) each consisting of 4 blocks averaging 5 min in length were counterbalanced throughout subjects (See Figure, 3).  The overarching pattern of stimulus presentation remained the same in each experimental condition and is according to the following: each trial began with a fixation cross which remained on screen for a random time interval between 2 and 4 seconds, followed by a tool-object image-fixation cross-tool-object image, and so on, until 60 novel tool-object images were displayed. Novel stimuli were used in the first two blocks of experimentation regardless of paradigm.  The amount of time a tool-object image remained on screen is dependent upon the block and experimental condition.   During the first paradigm (a), tool-object images were presented for 500 ms for blocks 1 and 2.  For the final 2 blocks, each tool-object image was presented for 3 s.  In paradigm (b), the only difference is that images during the first two blocks, blocks 1 and 2, tool-object images were displayed randomly for time intervals of either 500 ms or 2000 ms.  Again, during blocks 3 and 4 images were displayed for 3 s. Stimuli for blocks 3 and 4 consisted of randomized images from the previous two blocks (blocks 1 and 2) in both paradigms.
                </p>
            </div>
        </div>
        <div class="col-lg-12 mt-4">
          <img class="img-fluid mb-3 mb-lg-0 float-right" src="assets/img/paradigm-a-b.png" alt=""/>
        </div>
        <div class="col-lg-12">
          <p class="text-left pl-2 text-right"><b>Figure 3</b></p>
        </div>
    </div>

    <div class="row">
        <div class="col-md-12 mb-2" data-aos="" data-aos-delay="">
          <h5 class="text-muted">Image and video processing</h5>
          <p>
            Video, images, and behavioral data was overlaid using custom code written in Matlab.  Tool- object spaces were identified with edge detection.  Feature spaces included functional tool end, manipulative tool end, object space, static hand, and arm space.  After identifying feature spaces x,y eye tracking data points were then overlaid.  The following variables were analyzed: time for first saccade to reach a target, determining first saccade target, probability of saccade targets pertaining to feature identified feature spaces, and how long it took to generate a behavioral response after image presentation.
          </p>
      </div>
    </div>

    <div class="row">
        <div class="col-md-12 mb-5" data-aos="" data-aos-delay="">
          <h5 class="text-muted">Statistics</h5>
          <p>
            One-sample test tests were used to distinguish if behavioral response accuracy and saccadic eye movements are greater than chance (50%).  Spearman linear correlation analysis was used to determine correlations between probabilities.  One-way ANOAs, with LSD pos-hoc analysis was used to determine differences between functional, manipulative and no hand conditions. These were carried out in Matlab and SPSS.
          </p>
      </div>
    </div>

    <div class="row">
      <div class="col-md-12 mb-5" data-aos="" data-aos-delay="">
        <h3 class="text-left ">
          Results
        </h3>
        <p>
          Data analyzed are for the correct tool-object context with the following hand conditions: no hand, functional and manipulative (Row 1 of Figure 1, elements 1,3,4). Participants response accuracy was significantly greater than chance (81%, p<0.01).  During images presentation of 500 ms, participants also initiated saccades significantly more than chance (73.32%, p<0.1).
        </p>
      </div>
    </div>


    <div class="row align-items-center no-gutters mb-4 border-bottom" id="ourstory" >
        <div class="col-xl-4 col-lg-4 col-md-4">
            <div class="text-top text-left text-lg-left">
                <!-- <h5 class="text-muted">Stimuli</h5> -->
                <p class="mb-0">
            The longer it took a participant to initiate their first saccade, the more likely it became that they would fixate on the manipulative end of the tool.
            This correlation between time to initiate first saccade and probability to fixate on manipulative-tool-end feature space was found to be statistically significant (Figure 4).
                </p>
            </div>
        </div>
        <div class="col-xl-4 col-lg-4 col-md-4 mt-1">
            <img class="img-fluid mb-lg-0 pl-5" src="assets/img/manipulative-tool-end.png" alt=""/>
        </div>
        <div class="col-xl-4 col-lg-4 col-md-4 mt-1">
          <img class="img-fluid mb-3 mb-lg-0 float-right" src="assets/img/linear-predictor-of-attention-bias.png" alt=""/>
        </div>
        <div class="col-lg-12">
          <p class="text-right pr-5"><b>Figure 4</b></p>
        </div>
    </div>

    <div class="row align-items-center no-gutters mb-4 border-bottom" id="ourstory" >
        <div class="col-xl-4 col-lg-4 col-md-4 mb-2">
            <div class="text-top text-left text-lg-left">
                <!-- <h5 class="text-muted">Stimuli</h5> -->
                <p class="mb-0">
When a tool was pictured with a manipulative grasp, it was three times more likely that the participant would make their first saccade to the manipulative-tool-end feature space (Figure 5 and 6).
                </p>
                <div>
                  <img class="img-fluid mb-lg-0 pl-5 float-left" src="assets/img/manipulative-grasp.png" alt=""/>
                </div>
            </div>
        </div>
        <div class="col-xl-4 col-lg-4 col-md-4 mt-1">
          <img class="img-fluid mb-3 mb-lg-0" src="assets/img/probability-of-target-space.png" alt=""/>
          <p class="text-left pl-2"><b>Figure 5</b></p>
        </div>
        <div class="col-xl-4 col-lg-4 col-md-4 mt-1">
          <img class="img-fluid mb-3 mb-lg-0" src="assets/img/probability-of-first-saccade-targets.png" alt=""/>
          <p class="text-left pl-2"><b>Figure 6</b></p>
        </div>
    </div>



    <div class="row align-items-center no-gutters mb-4 border-bottom" id="ourstory" >
        <div class="col-xl-6 col-lg-6 col-md-6">
            <div class="text-top text-left text-lg-left order-first">
                <!-- <h5 class="text-muted">Stimuli</h5> -->
                <p class="mb-0 pl-3">
A tool pictured with a manipulative grasp also influenced response time and it took participants significantly longer to respond while viewing images from this experimental condition (Figure 7).
                </p>
            </div>
        </div>
        <div class="col-xl-6 col-lg-6 col-md-6 mt-1 order-lg-first order-md-first">
          <img class="img-fluid mb-3 mb-lg-0" src="assets/img/response-time-vs-first-saccade-target.png" alt=""/>
          <p class="text-right pr-5"><b>Figure 7</b></p>
        </div>
    </div>


    <div class="row align-items-center no-gutters mb-5" id="ourstory" >
        <div class="col-xl-6 col-lg-6 col-md-6">
            <div class="text-top text-left text-lg-left">
                <!-- <h5 class="text-muted">Stimuli</h5> -->
                <p class="mb-0">
Significantly longer response times also correlated with a saccadic bias towards the manipulative end of the tool (Figure 8).
                </p>
            </div>
        </div>
        <div class="col-xl-6 col-lg-6 col-md-6 mt-1">
          <img class="img-fluid mb-3 mb-lg-0 pl-2" src="assets/img/behavioral-response.png" alt=""/>
          <p class="text-left pl-4"><b>Figure 8</b></p>
        </div>
    </div>

    <div class="row mb-5">
      <div class="col-md-12" data-aos="" data-aos-delay="">
        <h3 class="text-left ">
          Discussion
        </h3>
        <p>
          The speed of visual processing has been well documented.  In accordance, 500 ms was a sufficient amount of time make accurate perceptual behavioral responses, with participants responding correct on 81% of trials.  To accomplish this feat, participants generated saccades in the majority of trails (73.32%).  This provides evidence that saccadic eye movements are beneficial even when there’s an extremely limited amount of time, and every feature space cannot receive a fixation or saccade.
        </p>
        <p>
          Further results suggest that the longer it takes to initiate a saccade the higher participants weighted the manipulative end of the tool.  Therefore, the manipulative end of the tool becomes more important as stimulus presentation time dwindles.  This effect only reached significance in the manipulative grasp condition.  This suggests that viewing a manipulative grasp posture influences gaze differently than functional grasps and conditions that don’t involve a hand.
          Additionally, the opposite construct, a saccadic bias towards the object feature space did not reach significance in any of the conditions. Meaning, that objects do not influence gaze in the same manner as images containing a manipulative grasp.
        </p>
        <p>
          Grasp also influenced first saccade targets.  In manipulative conditions, participants were 3 times as likely to make their first saccade to the manipulative feature space relative to no hand, and functional grasp conditions.  Additionally, if participants only made one saccade it was twice as likely to be toward the manipulative tool space relative to other conditions.
        </p>
      </div>
    </div>

    <div class="row">
      <div class="col-md-12 mb-5" data-aos="" data-aos-delay="">
        <h3 class="text-left ">
          Conclusions and Future Work
        </h3>
        <p>
          The manipulative posture alone evoked greater foveal bias towards the manipulative tool end. It also delayed saccade initiation and was
          highly likely to be the target of the first saccade during delayed saccade initiation. The above gaze and attention processes predicted delayed behavioral response times, without inhibiting accuracy. This result can possibly explain why the manipulative grasp in the correct context evokes unique spatiotemporal neural activations and delayed behavioral responses. In future studies, we will aim to build a logistic neural network model to predict accuracy and response times based on gaze data.
        </p>
      </div>
    </div>

    <div class="row">
      <div class="col-md-12 mb-5" data-aos="" data-aos-delay="">
        <h3 class="text-left ">
          Acknowledgements
        </h3>
        <p>
          This research was supported by the Center for Behavioral Neuroscience and a B.R.A.I.N. program grant (NH R01GM08539)
        </p>
      </div>
    </div>

    <div class="row">
      <div class="col-md-12 mb-2" data-aos="" data-aos-delay="">
        <h3 class="text-left ">
          References
        </h3>

          <ul>
             <li>
               Borghi, A. M., Flumini, A., Natraj, N., & Wheaton, L. A. (2012). One hand, two objects: Emergence of affordance in contexts. Brain and Cognition, 80(1), 64-73.
             </li>
             <li>
              Chao, L. L., Weisberg, J., & Martin, A. (2002). Experience-dependent modulation of category-related cortical activity. Cerebral Cortex, 12(5), 545-551.
             </li>
             <li>
                Ellis, R., & Tucker, M. (2000). Micro-affordance: The potentiation of components of action by seen objects. British Journal of Psychology, 91(4), 451.
             </li>
             <li>
              Gollisch, T. (2009). Throwing a glance at the neural code: Rapid information transmission in the visual system. HFSP Journal, 3(1), 36-46. doi:10.2976/1.3027089
             </li>
             <li>
Hamm, J. P., Dyckman, K. A., Ethridge, L. E., McDowell, J. E., & Clementz, B. A. (2010). Preparatory activations across a distributed cortical network determine production of express saccades in humans. The Journal of Neuroscience, 30(21), 7350-7357.
             </li>
             <li>
Kellenbach, M. L., Brett, M., & Patterson, K. (2003). Actions speak louder than functions: The importance of manipulability and action in tool representation. Journal of Cognitive Neuroscience, 15(1), 30-46.
             </li>
             <li>
               Lewis, J. W. (2006). Cortical networks related to human use of tools. The Neuroscientist, 12(3), 211-231.
             </li>
             <li>
               Mizelle, J., & Wheaton, L. A. (2010). The neuroscience of storing and molding tool action concepts: How “plastic” is grounded cognition? Frontiers in Psychology, 1
             </li>
             <li>Mizelle, J., & Wheaton, L. A. (2010). Why is that hammer in my coffee? A multimodal imaging investigation of contextually based tool understanding. Frontiers in Human Neuroscience, 4</li>
             <li>
               Mizelle, J. C., & Wheaton, L. A. (2010). Neural activation for conceptual identification of correct versus incorrect tool–object pairs. Brain Research, 1354(0), 100-112. doi:10.1016/j.brainres.2010.07.059
             </li>
             <li>
               Mizelle, J. C., & Wheaton, L. A. (2011). Testing perceptual limits of functional units: Are there “automatic” tendencies to associate tools and objects? Neuroscience Letters, 488(1), 92-96. doi:10.1016/j.neulet.2010.11.009
             </li>
             <li>
               Rizzolatti, G., Fogassi, L., & Gallese, V. (2001). OPINION: Neurophysiological mechanisms underlying the understanding and imitation of action. Nature Reviews Neuroscience, 2(9), 661-670. doi:10.1038/35090060
             </li>
             <li>
               Roberts, K. L., & Humphreys, G. W. (2010). Action relationships concatenate representations of separate objects in the ventral visual system. NeuroImage, 52(4), 1541-1548. doi:10.1016/j.neuroimage.2010.05.044
             </li>
             <li>
               Shadmehr, R. (2010). Control of movements and temporal discounting of reward. Current Opinion in Neurobiology.
             </li>
             <li>
               Thura, D., Boussaoud, D., & Meunier, M. (2008). Hand position affects saccadic reaction times in monkeys and humans. Journal of Neurophysiology, 99(5), 2194-2202.
             </li>
             <li>
               Torralba, A., Oliva, A., Castelhano, M. S., & Henderson, J. M. (2006). Contextual guidance of eye movements and attention in real-world scenes: The role of global features in object search. Psychological Review, 113(4), 766.
             </li>
             <li>
               Van Rullen, R., & Thorpe, S. J. (2001). Rate coding versus temporal order coding: What the retinal ganglion cells tell the visual cortex. Neural Computation, 13(6), 1255-1283.
             </li>
          </ul>

      </div>
    </div>









          <!-- container -->
          </div>
        <!-- section -->
        </div>
      </section>
    </main>
    <!-- End #main -->


<!-- ======= Footer ======= -->
  <footer class="footer" role="contentinfo">
    <div class="container">
      <div class="row">
        <div class="col-sm-6">
          <!-- <p class="mb-1">&copy; Copyright MyPortfolio. All Rights Reserved</p> -->
          <p class="mt-2">Designed by Dan DeWitz</p>
          <!-- <div class="credits"> -->
            <!--
            All the links in the footer should remain intact.
            You can delete the links only if you purchased the pro version.
            Licensing information: https://bootstrapmade.com/license/
            Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=MyPortfolio
          -->
            <!-- Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a> -->
            <!-- dewitz.dan@gmail.com -->
          <!-- </div> -->
        </div>
        <div class="col-sm-6 social text-md-right">
          <a target="_blank" href="https://www.linkedin.com/in/dan-dewitz-7b819757/"><span class="icofont-linkedin"></span></a>
          <a target="_blank" href="https://github.com/dan-dewitz"><span class="icofont-github"></span></a>
          <a target="_blank" href="https://www.instagram.com/dddewitz/"><span class="icofont-instagram"></span></a>
        </div>
      </div>
    </div>
  </footer>

    <a href="#" class="back-to-top"><i class="icofont-simple-up"></i></a>

    <!-- Vendor JS Files -->
    <script src="assets/vendor/jquery/jquery.min.js"></script>
    <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/vendor/jquery.easing/jquery.easing.min.js"></script>
    <script src="assets/vendor/php-email-form/validate.js"></script>
    <script src="assets/vendor/aos/aos.js"></script>
    <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="assets/vendor/owl.carousel/owl.carousel.min.js"></script>

    <!-- Template Main JS File -->
    <script src="assets/js/main.js"></script>
  </body>
</html>
